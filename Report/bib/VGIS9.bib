Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Xiang2017,
abstract = {Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation. PoseCNN estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables PoseCNN to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the YCB-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further refine the poses, our approach achieves state-of-the-art results on the challenging OccludedLINEMOD dataset.},
archivePrefix = {arXiv},
arxivId = {1711.00199},
author = {Xiang, Yu and Schmidt, Tanner and Narayanan, Venkatraman and Fox, Dieter},
eprint = {1711.00199},
file = {::},
month = {nov},
title = {{PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes}},
url = {https://arxiv.org/abs/1711.00199},
year = {2017}
}
@article{Redmon2016,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1612.08242},
file = {::},
month = {dec},
title = {{YOLO9000: Better, Faster, Stronger}},
url = {http://arxiv.org/abs/1612.08242},
year = {2016}
}
@article{Yilmaz2006,
abstract = {The goal of this article is to review the state-of-the-art tracking methods, classify them into different categories , and identify new trends. Object tracking, in general, is a challenging problem. Difficulties in tracking objects can arise due to abrupt object motion, changing appearance patterns of both the object and the scene, nonrigid object structures, object-to-object and object-to-scene occlusions, and camera motion. Tracking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. Typically, assumptions are made to constrain the tracking problem in the context of a particular application. In this survey, we categorize the tracking methods on the basis of the object and motion representations used, provide detailed descriptions of representative methods in each category, and examine their pros and cons. Moreover, we discuss the important issues related to tracking including the use of appropriate image features, selection of motion models, and detection of objects.},
author = {{Acm Reference Format: Yilmaz}, A and Javed, O and Shah, M},
doi = {10.1145/1177352.1177355},
file = {::},
journal = {ACM Comput. Surv},
keywords = {I48 [Image Processing and Computer Vision]: Scene,contour evolution,feature selection,object detection,object representation,point tracking,shape tracking},
pages = {45},
title = {{Object tracking: A survey}},
url = {http://doi.acm.org/10.1145/1177352.1177355},
volume = {38},
year = {2006}
}
@techreport{Zhao,
abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles which combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy and optimization function, etc. In this paper, we provide a review on deep learning based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely Convolutional Neural Network (CNN). Then we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network based learning systems.},
archivePrefix = {arXiv},
arxivId = {1807.05511v1},
author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
eprint = {1807.05511v1},
file = {::},
keywords = {Index Terms-deep learning,neural network,object detection},
title = {{Object Detection with Deep Learning: A Review}},
url = {https://arxiv.org/pdf/1807.05511.pdf},
year = {2018}
}
@inproceedings{Gall2012,
abstract = {Object detection multi class detection Random forests object detection ...},
author = {Gall, Juergen and Razavi, Nima and {Van Gool}, Luc},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-34091-8_11},
isbn = {9783642340901},
issn = {03029743},
keywords = {Hough forest,multi-class object detection,random forest,regression forest},
pages = {243--263},
title = {{An introduction to random forests for multi-class object detection}},
url = {http://link.springer.com/10.1007/978-3-642-34091-8{\_}11},
volume = {7474 LNCS},
year = {2012}
}
@article{Redmon2015,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
eprint = {1506.02640},
file = {::},
month = {jun},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@inproceedings{Tan2014,
abstract = {In this paper, we address the problem of object tracking in intensity images and depth data. We propose a generic framework that can be used either for tracking 2D templates in intensity images or for tracking 3D objects in depth images. To overcome problems like partial occlusions, strong illumination changes and motion blur, that notoriously make energy minimization-based tracking methods get trapped in a local minimum, we propose a learning-based method that is robust to all these problems. We use random forests to learn the relation between the parameters that defines the object's motion, and the changes they induce on the image intensities or the point cloud of the template. It follows that, to track the template when it moves, we use the changes on the image intensities or point cloud to predict the parameters of this motion. Our algorithm has an extremely fast tracking performance running at less than 2 ms per frame, and is robust to partial occlusions. Moreover, it demonstrates robustness to strong illumination changes when tracking templates using intensity images, and robustness in tracking 3D objects from arbitrary viewpoints even in the presence of motion blur that causes missing or erroneous data in depth images. Extensive experimental evaluation and comparison to the related approaches strongly demonstrates the benefits of our method.},
author = {Tan, David Joseph and Ilic, Slobodan},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.157},
isbn = {9781479951178},
issn = {10636919},
month = {jun},
pages = {1202--1209},
publisher = {IEEE},
title = {{Multi-forest tracker: A Chameleon in tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909553},
year = {2014}
}
@inproceedings{Or-El2015,
abstract = {The popularity of low-cost RGB-D scanners is increasing on a daily basis. Nevertheless, existing scanners often cannot capture subtle details in the environment. We present a novel method to enhance the depth map by fusing the intensity and depth information to create more detailed range profiles. The lighting model we use can handle natural scene illumination. It is integrated in a shape from shading like technique to improve the visual fidelity of the reconstructed object. Unlike previous efforts in this do- main, the detailed geometry is calculated directly, without the need to explicitly find and integrate surface normals. In addition, the proposed method operates four orders of magnitude faster than the state of the art. Qualitative and quantitative visual and statistical evidence support the im- provement in the depth obtained by the suggested method.},
author = {Or-El, Roy and Rosman, Guy and Wetzler, Aaron and Kimmel, Ron and Bruckstein, Alfred M.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7299179},
isbn = {9781467369640},
issn = {10636919},
month = {jun},
pages = {5407--5416},
pmid = {7299179},
publisher = {IEEE},
title = {{RGBD-fusion: Real-time high precision depth recovery}},
url = {http://ieeexplore.ieee.org/document/7299179/},
volume = {07-12-June},
year = {2015}
}
@article{Redmon2018,
abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
archivePrefix = {arXiv},
arxivId = {1804.02767},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1804.02767},
file = {::},
month = {apr},
title = {{YOLOv3: An Incremental Improvement}},
url = {http://arxiv.org/abs/1804.02767},
year = {2018}
}
@article{Leal-Taixe2017,
archivePrefix = {arXiv},
arxivId = {1704.02781},
author = {Leal-Taix{\'{e}}, Laura and Milan, Anton and Schindler, Konrad and Cremers, Daniel and Reid, Ian and Roth, Stefan},
eprint = {1704.02781},
file = {::},
month = {apr},
title = {{Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking}},
url = {https://arxiv.org/abs/1704.02781},
year = {2017}
}
