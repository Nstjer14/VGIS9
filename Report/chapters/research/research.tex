\chapter{Background Research}\glsresetall
\label{cha:Research}
As a part of the implementation of a viable grasping point of objects in an environmental setting a research of state of the solutions of both object tracking and detection is necessary as well as researching full existing solutions.

\section{Object Detection and Tracking}
In order to get some understanding of the state of the art technologies used the research in split into multiple sections for different areas in the process of teaching a robot to pick groceries from a shelf.

\subsection{Object Detection}
The aim of generic object detection is to locate and classify an object in an image, sometimes including a bounding box on objects with a confidence of existence in the image. In general there are two approaches to object detection namely region proposal, which follows the traditional object detection pipeline and then classifying the each proposal into different object categories. The other approach is the regression or classification based approach by adopting a unified framework to achieve final results \citep{zhao}.
\cite{zhao} presents several solutions of both approaches and have also made a small roadmap of different popular solutions up until 2017. This is shown in \autoref{fig:deep_roadmap}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/deep_roadmap.pdf}
  \caption{Small roadmap of different popular solutions up until 2017 \citep{zhao}}
  \label{fig:deep_roadmap}
\end{figure}

Another way of doing object detection is using decision trees instead of deep learning as proposed by \cite{Gall2012}.

\subsubsection{Region Proposal}
The region proposal framework is a two step process. The framework firstly scans an image and then focuses in any regions in the image of interest. As shown in \autoref{fig:deep_roadmap} the R-CNN solution is one of the first in region proposal approach. The design has three stages in the process of object detection. Firstly a region proposal generation, generating 2000 region proposals for each image. Afterwards a \gls{cnn} is applied to extract features from warped or cropped regions, extracting a 4096-dimensional feature vector. From there classification and categorisation is done with pre-trained \gls{svm}s from multiple classes. The final bounding boxes are produced from adjusted scored regions using bounding box regression.

\subsubsection{Regression or Classification framework}
To compete with the time consumption of regional proposal the one-step framework based on global regression/classification is utilised by mapping straight from image pixels to bounding box coordinates and class probabilities.
This is the technique the framework \gls{yolo} applies. \gls{yolo} divides an input image into $S \times S$ grid, each cell is responsible for predicting the object centred in the cell.
The first \gls{yolo} framework has 24 convolutional layers and 2 \gls{fc} layers \citep{zhao}. The second version, referred to as both \gls{yolo}v2 and \gls{yolo}9000, uses the Darknet-19 model, which has 19 convolutional layers and 5 maxpooling layers. It is based on the Googlenet architecture \citep{Redmon2016}. At the point of deployment this framework has state of the art performance. The third iteration of the framework \gls{yolo}v3 with an update of the Darknet architecture increasing the size from 19 convolutional layers to 53. The newest \gls{yolo} framework is from early 2018 and is still one of the best in its category. In precision \cite{Redmon2018} states the framework does not perform as well as RetinaNet, but the speed of the framework is faster. \gls{yolo} is fast and lightweight enough to run in real time. It is mostly trained on the COCO and VOC datasets \citep{Redmon2018}.

\subsubsection{Decision Trees}
A decision tree consists of split nodes and leaves. The split nodes evaluate each image patch based on a set decision metric and pass the patch to either left or right in the tree. The leaf is the end of a tree and stores the statistics of the image patches which arrived during training of the tree. This is illustrated in \autoref{fig:decisiontree}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/decisiontree}
  \caption{Example of decision tree evaluation on image patch from bounding box \citep{Gall2012}}
  \label{fig:decisiontree}
\end{figure}

\cite{Gall2012} uses a random forest for multi class object detection. A random forest consist of a set amount of decision trees.
The detection problem becomes a distribution estimation problem, where the random forests allow to learn features and descriptors that are optimal for estimating the distributions with low uncertainty. These distributions may be desirable to restrict to Gaussians or Gaussian Mixture Models \citep{Gall2012}.

The amount of training data is the most crucial parameter for detection accuracy, as random forests are designed to handle large amounts of data. If it is small training sets extra verification steps may be needed \citep{Gall2012}.

\subsection{Tracking}
General object tracking is tracking an object in a video or a sequence of images, in form of e.g. the problem of estimating the trajectory of an object in the image plane as the object moves in the image sequence. This means, that the tracker consistently labels the tracked objects in the different frames of the video \citep{Yilmaz2006}. Situations often present tracking of more than one object at the time where a multi object tracking framework is needed. \cite{Leal-Taixe2017} has evaluated on 32 different state of the art trackers from no sooner than March 2017 on multiple objects. The evaluation is performed the \gls{mot}15 and \gls{mot}16 datasets. This is done to introduce a standardised benchmark and analyse the performance of the trackers \citep{Leal-Taixe2017}. \cite{Leal-Taixe2017} states there are six top-performing trackers with a multiple object tracking accuracy higher than 40\%. Of the top six trackers, two of them are using deep learning, one using Recurrent neural networks to encode appearance of pedestrians and the other is using deep matching. The common component of the top performers are strong affinity models \citep{Leal-Taixe2017}.

\subsubsection{Three Dimensional Object Tracking}
In order to utilise three dimensional video, another dimension needs to be added by introducing depth in the data. This is done by using a depth camera to record and colouring the video in accordance to depth.

\cite{Tan2014} propose a solution for both two dimensional template an three dimensional object tracking using multi forest decision trees. This is based on another objective function which relates the image intensities of a template and transformation parameters by the pseudo-inverse of a Jacobian matrix, but with the use of random forests instead of the Jacobian, making the method generalised to any input function and not constrained to two dimensional intensity images \citep{Tan2014}. The random forests are regression forests to learn how different values of the transformation function affect the input function. The training of forests begins by creating a training dataset with $n_\omega$ random values to transform the input to affect the computation of the sample points. They randomly select $n_r$ points from $n_s$ sample points of the template for constructing a tree to impose randomness. \cite{Tan2014} uses 100 trees for each 6 parameters with a grid enclosed on the model seen from the depth image, only the points which lie on the model are used as sample points. In training $50\,000$ depth images are rendered with different kinds of distortions, such as rotation and translation. This is done for each camera view, of which there are 42.

\subsubsection{Object Pose}
For a robot to be able to interact with an object, more information than the position in an image is needed. The orientation of the object is also important. \cite{Xiang2017} seeks to estimate the six degrees object pose with the network, PoseCNN. PoseCNN, is trained using RGB-D scans of 21 objects and evaluated on both the YCB dataset and the OccludedLINEMOD dataset, to evaluate the robustness towards occlusions. The network has three different tasks done in order to estimate a six dimensional pose. Firstly a semantic labelling, secondly a 3D translation estimation, and thirdly a 3D rotation regression \citep{Xiang2017}. This is illustrated in \autoref{fig:poseCNN}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.55\textwidth]{figures/poseCNN}
	\caption{Overview of the three steps performed in by the PoseCNN \citep{Xiang2017}}
	\label{fig:poseCNN}
\end{figure}

\subsection{Grasping Planning}
For a robot to interact with an object a grasping point on the tracked object is necessary. \cite{Mahler2017} proposes a solution of grasping planning in two different solutions based on the same deep learning model called \gls{gqcnn}. They have created their own dataset with $1\,500$ 3D object models fro training the network. Before training of the network for the second iteration Dex-Net 2.0, \cite{Mahler2017} analyse a dataset of $6.7$ million point clouds with grasping options which has been generated from 3D models in randomised poses on a table. For the third iteration, Dex-Net 3.0 is generated. $2.8$ million point clouds with suction grasps, and grasps robustness labels on $1\,500$ 3D objects models. The database is used to train the \gls{gqcnn} to classify suction grasp robustness. This is done with a model of the pneumatic suction gripper used with the robot when testing \citep{Dexnet3}. 

Dex-Net 2.0 is tested 3D printed objects designed to challenge the parallel gripper used in this iteration and 10 household objects \citep{Mahler2017}. Dex-Net 3.0 is tested on objects divided into three different categories of basic, typical, and adversarial. Both the object categories and the performance in the different groups are shown in \autoref{fig:dexnet3}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/dexnet3}
	\caption{Dex-Net 3.0 object categories and performance. Basic (e.g. prismatic objects), Typical, and Adversarial \citep{Dexnet3}}
	\label{fig:dexnet3}
\end{figure}